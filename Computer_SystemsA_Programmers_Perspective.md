# 第一章：计算机系统漫游

​	信息就是位+上下文。

## 程序被其他程序翻译成不同的格式

​	 **gcc** 编译器驱动程序读取源程序文件 *hello.c* ，并把它翻译成一个可执行目标文件 *hello* ，这个翻译过程课分为四个阶段完成，执行这四个阶段的程序（**预处理器**、**编译器**、**汇编器**和**链接器**）一起构成了编译系统。

```mermaid
graph LR
a(hello.c<br/>源程序<br/>文本) --预处理器<br/>cpp--> b(hello.i<br/>修改了的源程序<br/>文本)
b --编译器<br/>ccl--> c(hello.s<br/>汇编程序<br/>文本)
c --汇编器<br/>as--> d(hello.o<br/>可重定向目标程序<br/>二进制)
d --链接器<br/>ld--> e(hello<br/>可执行目标程序<br/>二进制)
f(printf.o) --链接器<br/>ld--> e

```



- 预处理阶段。**预处理器**（ **cpp** ）根据以字符 **#** 开头的命令，修改原始的 C 程序。比如 *#include<stdio.h>* 命令告诉预处理器读取系统头文件 *stdio.h* 的内容，并把它直接插入程序**文本**中。结果得到了另一个 C 程序，通常是以 **.i** 作为文件扩展名。

- 编译阶段。**编译器**（ **ccl** ）将文本文件 *hello.i* 翻译成**文本**文件 *hello.s* ，它包含一个汇编语言程序。该程序包含函数 *main* 的定义。

- 汇编阶段。**汇编器**（ **as** ）将 *hello.s* 翻译成机器语言指令，把这些指令打包成一种叫做*可重定向目标程序*的格式，并将结果保存在目标文件 *hello.o* 中。 *hello.o* 文件是一个**二进制文件**。

- 链接阶段。 *hello* 程序调用了 *printf* 函数，它是每个 C 编译器都提供的标准 C 库中的一个函数。 *printf* 函数存在于一个名为 *printf.o* 的单独的预编译好了的目标文件中，而这个文件必须以某种方式合并到我们的 *hello.o* 程序中。**链接器**（ **ld** ）就负责处理这种工作。结果得到 *hello* 文件，可以被加载到内存中，由系统执行。

## 处理器读并解释存储在内存中的指令

```mermaid
flowchart LR;
    subgraph CPU
    PC[PC]
    cache[高速缓存存储器]
    reg <--> zxjk[总线接口]
    reg[寄存器文件] <--> ALU[ALU]
    cache <--> zxjk
    cache <--> reg
    end
io[I/O<br/>桥] <--> iozx[I/O总线]
zxjk <--系统<br/>总线--> io 
iozx <--> usb[USB<br/>控制器]
键盘 --> usb
鼠标 --> usb
iozx <--> 图形适配器 --> 显示器
iozx <--> 磁盘控制器 <--> 磁盘
iozx <--> 扩展槽
io <--内存<br/>总线--> ram[主存储器]
```

​	主存是由一组**动态随机存取存储器**（ **DRAM** ）芯片组成的。利用**直接存储器存取**（ **DMA** ）技术，数据可以不通过处理器而直接从磁盘到达主存。 L1 和 L2 高速缓存是用一种叫做**静态随机访问存储器**（ **SRAM** ）的技术实现的，利用了高速缓存的**局部性原理**，即程序具有访问局部区域里的数据和代码的趋势。

​	所有应用程序对硬件的操作尝试都必须通过操作系统。操作系统有两个基本功能：（1）防止硬件被失控的应用程序滥用；（2）向应用程序提供简单一致的机制来控制复杂而又通常大不相同的低级硬件设备。**文件**是对 I/O 设备的抽象表示，**虚拟内存**是对主存和磁盘 I/O 设备的抽象表示，**进程**则是对处理器、主存和 I/O 设备的抽象表示。

​	**进程**是操作系统对一个正在运行的程序的一种抽象。在一个系统上可以同时运行多个进程，而每个进程都好像在独占地使用硬件。而**并发运行**，则是说一个进程的指令和另一个进程的指令是交错执行的。无论是在单核还是多核系统中，一个 CPU 看上去都像是在并发地执行多个进程，这是通过处理器在进程间切换来实现的。操作系统实现这种交错执行的机制称为**上下文切换**。

​	操作系统保持跟踪进程运行所需的所有状态信息。这种状态，也就是**上下文**，包括许多信息，比如 **PC** 和**寄存器**文件的当前值，以及**主存**的内容。在任何一个时刻，单处理器系统都只能执行一个进程的代码。当操作系统决定要把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文、恢复新进程的上下文，然后将控制权传递到新进程。

​	从一个进程到另一个进程的转换是由操作系统**内核**管理的。内核是操作系统代码常驻主存的部分。当应用程序需要操作系统的某些操作时，比如读写文件，它就执行一条特殊的**系统调用** (system call) 指令，将控制权传递给内核。然后内核执行被请求的操作并返回应用程序。注意，内核不是一个独立的进程。相反，它是<span style="background-color: yellow;">系统管理全部进程所用代码和数据结构的集合</span>。

​	在现代系统中，一个进程实际上可以由多个称为**线程**的执行单元组成，每个线程都运行在进程的上下文中，并**共享**同样的**代码**和**全局数据**。由于网络服务器中对并行处理的需求，线程成为越来越重要的编程模型，因为<span style="background-color: yellow;">多线程之间比多进程之间更容易共享数据</span>，也因为线程一般来说都比进程更高效。

- **本质区别：**进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位。
- **包含关系：**一个进程至少有一个线程，线程是进程的一部分，所以线程也被称为轻权进程或者轻量级进程。
- **资源开销：**每个进程都有独立的地址空间，进程之间的切换会有较大的开销；线程可以看做轻量级的进程，同一个进程内的线程共享进程的地址空间，每个线程都有自己独立的运行栈和程序计数器，线程之间切换的开销小。
- **影响关系：**一个进程崩溃后，在保护模式下其他进程不会被影响，但是一个线程崩溃可能导致整个进程被操作系统杀掉，所以多进程要比多线程健壮。

[线程之间共享了哪些资源？](https://zhuanlan.zhihu.com/p/519803985)

> ​    函数运行时的信息保存在栈帧中，栈帧中保存了函数的返回值、调用其它函数的参数、该函数使用的局部变量以及该函数使用的寄存器信息，此外， CPU 执行指令的信息保存在一个叫做程序计数器的寄存器中，通过这个寄存器我们就知道接下来要执行哪一条指令。由于操作系统随时可以暂停线程的运行，因此我们保存以及恢复程序计数器中的值就能知道线程是从哪里暂停的以及该从哪里继续运行了。
>
> ​    由于线程运行的本质就是函数运行，函数运行时信息是保存在栈帧中的，因此每个线程都有自己独立的、私有的栈区。同时函数运行时需要额外的寄存器来保存一些信息，像部分局部变量之类。这些寄存器也是线程私有的，一个线程不可能访问到另一个线程的这类寄存器信息。
>
> ​    从上面的讨论中我们知道，到目前为止，所属<span style="background-color: yellow;">线程的栈区、程序计数器、栈指针以及函数运行使用的寄存器</span>是线程私有的。以上这些信息有一个统一的名字，就是**线程上下文**。
>
> ​    我们也说过操作系统调度线程需要随时中断线程的运行并且需要线程被暂停后可以继续运行，操作系统之所以能实现这一点，依靠的就是线程上下文信息。除此之外，剩下的都是线程间共享资源。包括**代码区**、**数据区**和**堆区**。

​	**虚拟内存**是一个抽象概念，它为每个进程提供了一个假象，即每个进程都在独占地使用主存。每个进程看到的内存都是一致的，称为**虚拟地址空间**。 Linux 中，地址空间最上面的区域是保留给操作系统中的代码和数据的，这对所有进程来说都是一样。地址空间的底部区域存放用户进程定义的代码和数据。

每个进程看到的虚拟地址空间由大量准确定义的区构成，每个区都有专门的功能。从最低的地址开始，逐步向上介绍。

- **程序代码和数据**。对所有的进程来说，代码是从同一固定地址开始，紧接着的是和全局变量相对应的数据位置。代码和数据区是直接按照可执行目标文件的内容初始化的。

- **堆**。代码和数据区后紧随着的是运行时堆。代码和数据区在进程一开始运行时就被指定了大小，与此不同，当调用像 *malloc* 和 *free* 这样的 C 标准库函数时，堆可以在运行时动态地扩展和收缩。

- **共享库**。大约在地址空间的中间部分是一块用来存放像 C 标准库和数学库这样的共享库的代码和数据的区域。

- **栈**。位于用户虚拟地址空间顶部的是**用户栈**，编译器用它来实现函数调用。和堆一样，用户栈在程序执行期间可以动态地扩展和收缩。特别地，每次我们调用一个函数时，栈就会增长；从一个函数返回时，栈就会收缩。

- **内核虚拟内存**。地址空间顶部的区域是为内核保留的。不允许应用程序读写这个区域的内容或者直接调用内核代码定义的函数。相反，它们必须调用内核来执行这些操作。

​	虚拟内存的运作需要硬件和操作系统软件之间精密复杂的交互，包括对处理器生成的每个地址的硬件翻译。基本思想是把一个进程虚拟内存的内容存储在磁盘上，然后用主存作为磁盘的高速缓存。

​	**文件**就是字节序列，每个 I/O 设备，包括磁盘、键盘、显示器，甚至网络，都可以看成是文件。系统中的所有输入输出都是使用一小组称为 **Unix I/O** 的系统函数调用读写文件来实现的。文件向应用程序提供了一个统一的视图，来看待系统中可能含有的所有各式各样的 I/O 设备。

<span style="background-color: yellow;">**Amdahl 定律**（阿姆达尔定律）</span>：考量提升系统性能的效果

> ​	主要思想是，当我们对系统的某个部分加速时，其对系统整体性能的影响取决于该部分的**重要性**和**加速程度**。
>
> ​	若系统执行某应用程序需要的时间为 $T_{old}$ ，假设系统某部分所需执行时间与该时间占比为 $\alpha$ ，而该部分性能提升比例为 $k$ ，因此，总的执行时间为 $T_{new} = (1-\alpha)T_{old}+(\alpha T_{old})/k=T_{old}[(1-\alpha)+\alpha/k]$，因此，可以计算出加速比为
> $$
> S=\frac{T_{old}}{T_{new}}=\frac{1}{(1-\alpha)+\alpha/k}
> $$

按照系统层次由高到低的顺序重点强调三个层次的并发：

1. **线程级并发**

   自 20 世纪 60 年代初期出现时间共享以来，计算机系统就开始有了对并发执行的支持。传统配置的**单处理器系统**是通过正在执行的线程间快速切换来实现的。随着**多核处理器**和**超线程**的出现，**多处理器系统**变得更加常见。

   > 超线程技术与多核体系结构的区别如下：①超线程技术是通过延迟隐藏的方法，提高了处理器的性能，本质上，就是多个线程共享一个处理单元。因此，采用超线程技术所获得的性能并不是真正意义上的并行。从而采用超线程技术获得的性能提升，将会随着应用程序以及硬件平台的不同而参差不齐。②多核处理器是将两个甚至更多的独立执行单元，嵌入到一个处理器内部。每个指令序列（线程），都具有一个完整的硬件执行环境，所以各线程之间就实现了真正意义上的并行。

2. **指令级并发**

   **流水线**（旁注：添加跳转到第四章）的使用，可以把一条指令所需要的活动划分成不同的步骤，可以并行地操作。如果处理器可以达到比一个周期一条指令更快的执行效率，就称之为**超标量处理器**。

3. **单指令、多数据并行**

   提供这些 **SIMD** 指令多是为了提高处理影像、声音和视频数据应用的执行速度。可靠的方法是用编译器支持的特殊的向量数据类型来写程序。

# 第四章：处理器体系结构

## 指令集体系结构

​	一个处理器支持的指令和指令的字节级编码称为它的**指令集体系结构（ ISA ）**。 ISA 在编译器编写者和处理器设计人员之间提供了一个概念抽象层，编译器编写者只需要指导允许哪些指令，以及是如何编码的；而处理器设计者必须建造出执行这些指令的处理器。定义一个指令集体系结构包括定义各种**状态单元**、**指令集**和它们的**编码**、一组**编程规范**和**异常事件处理**。

​	程序中的每条指令都会读取或修改处理器状态的某些部分，这称为**程序员可见状态**，包括数个**程序寄存器**、四个**一位**的[**条件码**](#条件码)、**程序计数器（ PC ）**、**程序状态**和**内存**。内存从概念上来说就是一个很大的字节数组，保存着程序和数据。我们假设用**虚拟地址**来引用内存位置。硬件和操作系统软件联合起来将虚拟地址翻译成实际或**物理地址**。状态码表明程序执行的总体状态，会指示是正常运行还是出现了某种异常。

<a id="条件码">条件码</a>:

> 1. **CF**：进位标志。最近的操作使最高位产生了进位。可用来检查无符号操作的溢出；
> 2. **ZF**：零标志。最近的操作得出的结果为0；
> 3. **SF**：符号标志。最近的操作得到的结果为负数；
> 4. **OF**：溢出标志。最近的操作导致一个补码溢出——正溢出或负溢出。
>
>    例如XOR，进位标志和溢出标志会设置成0。对于移位操作，进位标志将被设置为最后一个被移出的位，而溢出标志设置为0。INC和DEC指令会设置溢出和零标志。

​	 15 个程序寄存器中每个都有一个相对应的范围在 0 到 *0xE* 之间的**寄存器标识符**。程序寄存器存在 CPU 中的一个**寄存器文件**中，这个寄存器文件就是一个小的、以寄存器 ID 作为地址的随机访问存储器。在指令编码以及我们的硬件设计中，当需要指明不应访问任何寄存器时，就用 ID 值 *0xF* 来表示。 

​	 *x86-64* 有时称为“复杂指令集计算机”（CISC），与“精简指令集计算机”（RISC）相对。以下是两者的一些区别：

|               CISC               |                  RISC                  |
| :------------------------------: | :------------------------------------: |
|             指令很多             |      指令数量少，通常少于 100 个       |
| 编码是可变长度的，可以是1~15字节 |         编码固定长度为 4 字节          |
|   可以对内存进行算术和逻辑运算   |     允许内存使用的只有load和store      |
|  有条件码，可以用于条件分支检测  | 没有条件码，会将测试结果保存到寄存器中 |
|         栈密集的过程衔接         |          寄存器密集的过程衔接          |

## 逻辑设计和硬件控制语言 HCL

​	要实现一个数字系统需要三个主要的组成部分：计算对位进行操作的函数的组合逻辑、存储位的存储器单元，以及控制存储器单元更新的时钟信号。

​	逻辑门是数字电路的基本计算单元，将很多的逻辑门组合成一个网，构建计算块，称为**组合电路**。有以下限制：

1. 每个逻辑门的输入必须连接到下述选项之一：`1）`一个系统输入（称为主输入）；`2）`某个存储器单元的输出；`3）`某个逻辑门的输出。
2. 两个或多个逻辑门的输出不能连接到一起。
3. 这个网必须是无欢的。

​	一个简单但很有用的组合电路，称为**多路复用器**，在单个位的多路复用器中，两个数据信号是输入 a 和 b ，控制信号输入是 s 。当 s 是 1 时，输出为 a ，反之输出为 b 。

  ```c++
bool out = (s && a) || (!s && b);
  ```

​	类似地，我们也可以设计出字级的多路复用器，由多个相同的子电路组成。值得注意的是，在**硬件控制语言（ HCL ）**中的逻辑表达式中是没有短路求值的。

​	组合电路从本质上来讲，不存储任何信息。它们只是简单地响应输入信号，产生等于输入的某个函数的输出。为了产生**时序电路**，必须引入按位存储信息的设备。存储设备都是由同一个**时钟**控制的，时钟是一个周期性信号。考虑两种存储器设备：

- 时钟寄存器（简称**寄存器**）存储某个位或字。时钟信号控制寄存器加载输入值。
- 随机访问存储器（简称**内存**）存储多个字，用地址来选择读写哪个字。例子有：`1）`处理器的虚拟内存系统；`2）`寄存器文件。

## 流水线的通用原理

​	流水线提高了系统的**吞吐量**，但也会轻微地增加**延迟**。

# 第六章：存储器层次结构

​	存储器系统是一个具有不同容量、成本和访问时间的存储器设备的层次设备。

## 存储技术

​	随机访问存储器（Random-Access Memory，RAM）分为两大类：静态的和动态的。静态 RAM （ SRAM ）比动态 RAM （ DRAM ）更快，但也贵得多。相对的来说， SRAM 的存储空间小于 DRAM。

1. 静态 RAM

​	 SRAM 将每个位存储在一个双稳态的存储器单元内。每个单元是用一个六晶体管电路来实现的。这个电路有这样一个属性，它可以无限期地保持在两个不同的电压配置或状态之一。其他任何状态都是不稳定的。由于 SRAM 存储器单元的双稳态特性，只要有电，它就会永远地保持它的值。即使有干扰，例如电子噪声，来扰乱电压，当干扰消除时，电路就会恢复到稳定值。

2. 动态 RAM

​	 DRAM 将每个位存储为对一个电容的充电。这个电容非常小。 DRAM 存储器可以制造得非常密集，每个单元由一个电容和一个访问晶体管组成。但是，不同的是， DRAM 存储单元对干扰非常敏感。当电容的电压被扰乱后，它就永远不会恢复了。暴露在阳光下会导致电容电压的变化，可以制作成数码相机的传感器。

​	很多原因会导致漏电，但是计算机运行的时钟周期远小于失去电荷的时间。内存系统必须周期性地通过读出，然后重写来刷新内存中的每一位。有些系统也使用纠错码。

3. 传统的 DRAM

​	 DRAM 芯片中的单元被分为 $d$ 个**超单元**，每个超单元都由 $w$ 个 DRAM 单元组成。超单元被组织成一个 $r$ 行 $c$ 列的长方形阵列，这里 $rc=d$ 。每个超单元都有形如 $(i,j)$ 的地址。信息通过称为**引脚**的外部连接器流入和流出芯片。每个引脚携带一个 1 位的信号。每个 DRAM 芯片被连接到某个称为**内存控制器**的电路，这个电路可以一次传送 $w$ 位到每个 DRAM 芯片或一次从每个 DRAM 芯片传出 $w$ 位。为了读出超单元 $(i,j)$ 的内容，内存控制器将行地址 $i$ 发送给 DRAM ，然后是列地址 $j$ 。 DRAM 把超单元 $(i,j)$ 的内容发送回控制器作为响应。会首先将整一行的内容复制到一个内部行缓冲区，接下来收到列地址后，再发送具体的内容到内存控制器。行地址 $i$ 和列地址 $j$ 称为 RAS （Row Access Strobe，行访问选通脉冲）请求和 CAS（Column Access Strobe，列访问选通脉冲）请求，共享相同的 DRAM 地址引脚。二维阵列而不是线型数组的一个原因是降低芯片上的地址引脚数量，但必须分两次发送地址，增加了访问时间。

4. 增强的 DRAM

- 快页模式 DRAM （Fast Page DRAM， FPM DRAM）。传统的 DRAM 将超单元的一整行复制到它的内部行缓冲区中，使用一个，然后丢弃剩余的。 FPM DRAM 允许对同一行连续地访问可以直接从行缓冲区得到服务。
- 扩展数据输出 DRAM （Extended Data Out DRAM，EDO DRAM）。 FPM DRAM 的一个增强的形式，允许各个 CAS 信号在时间上靠的更紧密一些。
- 同步 DRAM（Synchronous DRAM，SDRAM）。用与驱动内存控制器相同的外部时钟信号的上升沿来替代许多控制信号。能够比异步的存储器更快地输出它的超单元的内容。
- 双倍数据速率同步 DRAM（Double Data-Rate Synchronous DRAM，DDR SDRAM），是对 SDRAM 的一种增强，它使用两个时钟沿作为控制信号，从而使速度翻倍。不同类型的 DDR SDRAM 是用提高有效带宽的很小的预取缓冲区的大小来划分的： DDR （ 2 位）、 DDR2 （ 4 位）、 DDR4 （ 8 位）。
- 视频 RAM （Video RAM，VRAM）。它用在图形系统的帧缓冲区中。 VRAM 的思想与 FPM DRAM 类似。两个主要的区别是：1） VRAM 的输出是通过依次对内部缓冲区的整个内容进行位移得到的；2） VRAM 允许对内存并行地进行读和写。因此，系统可以在写下一次更新的新值的同时，用帧缓冲区中的像素刷屏幕。

5. 非易失性存储器

​	非易失性存储器即使是在关闭电源后，仍然保存着信息。**只读存储器**（Read-Only Memory，ROM）是以它们能够被重编程（写）的次数和对它们进行重编程所用的机制进行区分，并不是只能读不能写。 PROM （Programmable ROM，**可编程 ROM** ）只能被编程一次。 PROM 的每个存储器单元都有一种熔丝，只能用高电流熔断一次。**可擦写可编程 ROM **（Erasable Programmable ROM，EPROM）有一个透明的石英窗口，允许光到达存储单元。紫外线光照射进窗口， EPROM 单元就被清除为 0 。对 EPROM 编程是通过使用一种把 1 写入 EPROM 的特殊设备来完成的。能够被擦除和重编程的次数的数量级可以达到数千次。**电子可擦除 PROM **（Electrically Erasable PROM，EEPROM）类似于 EPROM，但是它不需要一个物理上独立的编程设备，因此可以直接在印刷电路卡上编程。其能够被编程的次数的数量级达到十万次。

​	**闪存**是一类非易失性存储器，基于 EEPROM ，提供快速而持久的非易失性存储。**固态硬盘**是一种新型的基于闪存的磁盘驱动器，能提供相对于传统旋转磁盘的一种更快速、更强健和更低能耗的选择。

​	存储在 ROM 设备中的程序通常被称为**固件**。当一个计算机系统通电后，会运行存储在 ROM 中的固件。一些系统在固件中提供了少量基本的输入和输出函数——例如 PC 的 BIOS （基本输入、输出系统）例程。复杂的设备，像图形卡和磁盘驱动控制器，也依赖固件翻译来自 CPU 的 I/O 请求。

---

​	数据流通过**总线**在处理器和 DRAM 主存之间流通。每次数据传送都是通过称为**总线事务**的一系列步骤实现的。 CPU 通过系统总线连接到 **I/O 桥**（包含内存控制器）， I/O 桥通过内存总线与主存连接。

​	执行加载操作时， CPU 芯片上称为总线接口的电路在总线上发起读事务，由三个步骤组成。首先， CPU 将地址放到系统总线上， I/O 桥将信号传递到内存总线。接下来，主存感觉到内存总线上的地址信号，将数据写到内存总线。 I/O 桥将内存总线信号翻译成系统总线信号，然后沿着系统总线传递回 CPU。

​	执行存储操作时， CPU 发起写事务。同样，也有三个步骤。首先， CPU 将地址放到系统总线上，内存从内存总线上读出地址，并等待数据到达。接下来， CPU 将寄存器里的值复制到系统总线。最后，主存从内存总线中读出数据，存储到 DRAM 中。

## 磁盘存储

​	磁盘上读信息的时间为毫秒级，比从 DRAM 读满了十万倍，比从 SRAM 读慢了百万倍。磁盘用**读/写头**来读写存储在磁性表面的位，有多个盘面的磁盘针对每个盘都有一个独立的读/写头，垂直排列，一致行动，在任何时刻，所有的读/写头都位于同一个柱面上。

​	磁盘以扇区大小的块来读写数据。对扇区的访问时间有三个主要的部分：寻道时间、旋转时间和传送时间。

- **寻道时间**：移动传动臂所需的时间称为寻道时间，是通过对几千次对随机扇区的寻道求平均值来测量的，通常为 3~9 ms，最大时间可以高达 20 ms。
- **旋转时间**：到达期望的磁道后等待目标扇区旋转到读/写头下。
- **传送时间**：一个扇区的传送时间依赖于旋转速度和每条磁道的扇区数目。

​	访问一个磁盘扇区的时间主要是寻道时间和旋转延迟，访问扇区中的第一个字节用了很多时间。因为寻道时间和旋转延迟大致相等，一般用寻道时间乘 2 是估计磁盘访问时间的简单而合理的方法。

​	硬盘封装中有一个小的硬件/固件设备，称为**磁盘控制器**，维护着逻辑块号和实际（物理）磁盘扇区之间的映射关系。

​	 CPU 使用一种称为**内存映射 I/O **的技术来向 I/O 设备发射命令。在使用内存映射 I/O 的系统中，地址空间中有一块地址是为与 I/O 设备通信保留的。每个这样的地址称为一个 **I/O 端口**。当一个设备连接到总线时，它与一个或多个端口相关联（或它被映射到一个或多个端口）。

​	来看一个简单的例子，假设磁盘控制器映射到端口 0xa0 。随后， CPU 可能通过执行三个对地址 0xa0 的存储指令，发起磁盘读：第一条指令是发送一个命令字，告诉磁盘发起一个读，同时还发送了其他的参数，例如当读完的时候，是否中断 CPU 。第二条指令指明应该读的逻辑块号。第三条指令指明应该存储磁盘扇区内容的主存地址。当 CPU 发出了请求之后，在磁盘执行读的时候，通常会做些其他的工作。在传输进行时，只是简单的等待，是一种极大的浪费。

​	在磁盘控制器收到来自 CPU 的读命令后，它将逻辑块号翻译成一个扇区地址，读该扇区的内容，然后将这些内容直接传送到主存，不需要 CPU 的干涉。设备可以自己执行读或者写总线事务而不需要 CPU 干涉的过程，称为**直接内存访问**（Direct Memory Access，DMA）。这种数据传送称为 DMA 传送。在 DMA 传送完成，磁盘扇区的内容被安全地存储在主存中以后，磁盘控制器通过给 CPU 发送一个中断信号来通知 CPU 。基本思想是中断会发信号到 CPU 芯片的一个外部引脚上。这会导致 CPU 暂停它当前正在做的工作，跳转到一个操作系统例程。这个程序会记录下 I/O 已经完成，然后将控制返回到 CPU 被中断的地方。

---

​	固态硬盘（Solid State Disk，SSD）是一种基于闪存的存储技术。一个 SSD 封装由一个或多个闪存芯片和**闪存翻译层**组成，闪存芯片替代传统旋转磁盘中的机械驱动盘，而闪存翻译层是一个硬件/固件设备，扮演和磁盘控制器相同的角色，将对逻辑块的请求翻译成对底层物理设备的访问。读 SSD 略快于写。

​	随机读和写的性能差别是由底层闪存基本属性决定的。一个闪存由 $B$ 个块的序列组成，每个块由 $P$ 页组成。通常页的大小是 512 字节 ~ 4 KB ，块是由 32 ~ 128 页组成的。数据是以页为单位读写的。只有在一页所属的块被整个擦除后，才能写这一页（通常指该块中的所有位都被设置成 1 ），在大约十万次重复写之后，块就会磨损坏，不能再使用了。

​	随机写很慢，有两个原因。首先，擦除块需要相对较长的时间， 1 ms 级的，比访问页所需时间要高一个数量级。其次，如果写操作试图修改一个包含已经有数据（也就是不是全为 1 ）的页，那么这个块中所有带有用数据的页都必须被复制到一个新（擦出过的）块，然后才能对页进行写。制造商已经在闪存翻译层实现了复杂的逻辑，试图抵消擦写块的高昂代价，最小化内部写的次数，但是随机的写不可能和读一样的好。

​	比起旋转磁盘， SSD 有很多优点：它们由半导体存储器构成，没有移动的部件，因而随机访问时间比旋转磁盘要快，能耗更低，同时也更结实。不过也有一些缺点：首先， SSD 更容易磨损。闪存翻译层中的**平均磨损**逻辑试图通过将擦除平均分布在所有的块上来最大化每个块的寿命。实际上，这处理得非常好。其次， SSD 每字节贵大约三十倍。

## 存储器层次结构中的缓存

​	如果缓存中没有寻找到数据对象，即没有**缓存命中**，通过缓存的**替换策略**驱逐一个块填写数据对象所在的块。只要发生了不命中，缓存就必须执行某个**放置策略**，确定把取出的块放在哪里，最灵活的策略就是允许上层的任何块放在本层的任何块里，这个策略实现起来通常很昂贵，因为随机地放置块，定位起来代价很高。通常使用的是更严格的放置策略。

## 高速缓存存储器

​	考虑一个计算机系统，其中每个存储器地址有 $m$ 位，形成 $2^m$ 个不同的地址，这样一个机器的高速缓存被组织成一个有 $S=2^s$ 个**高速缓存组**的数组，每个组包含 $E$ 个高速缓存行。每个行是由一个 $B=2^b$ 字节的数据块组成的，一个有效位指明这个行是否包含有意义的信息，还有 $t=m-(b+s)$ 个标记位，它们唯一地标识存储在这个高速缓存行的块。一般而言，高速缓存的结构可以用元组 $(S,E,B,m)$ 来描述。高速缓存的大小 $C$ 就是所有块的大小之和，标记位和有效位不包含在内，因此 $C = S\times E\times B$ 。

​	当一条加载指令指示 CPU 从主存地址 $A$ 中读一个字时，它将地址 $A$ 发送给高速缓存。如果高速缓存正保存着那个字的副本，它就立刻将那个字发回给 CPU 。那么高速缓存是如何知道自己是否包含地址 $A$ 的字：

​	参数 $S$ 和 $B$ 将 $m$ 个地址分成了三个字段 $[m-1:0]=[s+b+t-1:s+b]|[s+b-1:b]|[b-1:0]$ 。 $A$ 中 $s$ 个**组索引位**是一个到 $S$ 个组的数组的索引，被解释成一个无符号整数，告诉这个字必须存储在哪个组中。 $A$ 中的 $t$ 个**标记位**告诉我们这个组中的哪一行包含这个字。当且仅当设置了有效位并且该行的标记位与地址 $A$ 中的标记位相匹配时，组中的这一行才包含这个字，然后 $b$ 个**块偏移位**给出了在 $B$ 个字节的数据块中的字偏移。

<style>
  table{
    border-collapse: collapse;
  }
  th,td{
    border: 1px solid black;
    padding: 10px;
    text-align: center;
  }
</style>
<table>
<tr>
<td colspan =  "2"> 基本参数</td>
</tr>
<tr>
<td>参数</td>
<td>描述</td>
</tr>
<tr>
<td>S</td>
<td>组数</td>
</tr>
<tr>
<td>E</td>
<td>每组的行数</td>
</tr>
<tr>
<td>B</td>
<td>块大小（字节）</td>
</tr>
<tr>
<td>m</td>
<td>（主存）物理地址位数</td>
</tr>
<tr>
<td>s</td>
<td>组索引位数量</td>
</tr>
<tr>
<td>b</td>
<td>块偏移位数量</td>
</tr>
<tr>
<td>t=m-(s+b)</td>
<td>标记位数量</td>
</tr>
<tr>
<td>C</td>
<td>高速缓存大小（字节）</td>
</tr>
</table>

​	根据每个组的高速缓存行数 $E$ ，高速缓存被分为不同的类。每组只有一行（ $E=1$ ）的称为**直接映射**高速缓存。很容易发生冲突不命中的问题。一组多行的高速缓存称为**组相联**高速缓存。**全相联**高速缓存是由一个包含所有高速缓存行的组组成的。因为高速缓存电路必须并行地搜索许多相匹配的标记，构造一个又大又快的相联高速缓存很困难，也很昂贵。因此只适合做小的高速缓存，例如虚拟内存系统中的翻译备用缓冲器，它缓存页表项。

​	写的情况更为复杂。假设要写一个已经缓存了的字 $w$ ，在高速缓存中更新了它的副本之后，如何更新 $w$ 在层次结构中紧接着第一层中的副本。最简单的方法就是**直写**。立即将 $w$ 的高速缓存块写回低一层去，虽然简单，但是缺点是每次写都会引起总线流量。另一种方法，称为**写回**，尽可能地推迟更新，只有当替换算法要驱逐这个更新过的块时，才把它写到低一层去。由于局部性，写回能显著地减少总线流量，但是缺点是增加了复杂性。必须为每个高速缓存行维护一个额外的**修改位**，表明是否被修改过。

​	另一个问题是如何处理写不命中。一种方法称为**写分配**，加载相应的低一层的块到高速缓存中，然后更新这个高速缓存块。写分配试图利用的写的空间局部性，但是缺点是每次不命中都会导致一个块从低一层传递到高速缓存。另一种方法称为**非写分配**，避开高速缓存，直接把这个字写到低一层去。直写高速缓存通常是非写分配的。写回高速缓存通常是写分配的。

​	实际上，高速缓存既保存数据，也保存指令。只保存指令的高速缓存称为 i-cache ，只保存程序数据的高速缓存称为 d-cache ，两者都保存的高速缓存称为**统一的**高速缓存。现代处理器包括独立的 i-cache 和 d-cache 。 i-cache 通常是只读的，因此比较简单，会针对不同的访问模式来优化。

1. 高速缓存大小的影响

   ​	一方面，较大的缓存可能提高命中率。另一方面，使大存储器运行更快总是困难的，可能会增加命中时间。

2. 块大小的影响

   ​	大的块有利有弊。一方面，较大的块能利用程序中可能存在的空间局部性，帮助提高命中率。不过，对于给定的高速缓存大小，块越大意味着高速缓存行数越少，这会损害时间局部性比空间局部性更好的程序中的命中率。较大的块对不命中的处罚也有负面影响，因为块越大，传送时间越长。现代系统中会折中包含 64 个字节。

3. 相联度的影响

   ​	这里的问题是参数 $E$ 选择的影响。较高的相联度的优点是降低了高速缓存由于冲突不命中出现**抖动**的可能性。不过也会造成较高的成本，实现起来昂贵且很难使其速度变快。每一行需要更多的标记位，需要额外的 LRU 状态位和额外的控制逻辑。会增加命中时间，因为复杂性增加了。另外也会增加不命中的处罚，因为选择牺牲行的复杂性也增加了。

   ​	相联度的选择最终变成了命中时间和不命中处罚的折中。传统上，努力争取时钟频率的高性能系统会为 L1 高速缓存选择较低的相联度，而在不命中处罚比较高的较低层上使用比较大的相联度。

   ​	一个程序从存储系统中读数据的速率称为**读吞吐量**，如果我们从一个紧密程序循环中发出一系列读请求，那么测量出的读吞吐量能让我们看到对于这个读序列来说的存储系统的性能。如下给出了一对测量某个读序列的读吞吐量的函数。

   <img src="./pic/image-20231013001408304.png" alt="image-20231013001408304" style="zoom: 50%;" />
	
	<img src="./pic/image-20231013001441868.png" alt="image-20231013001441868" style="zoom:50%;" />
	
	​	 *run* 函数的参数 *size* 和 *stride* 允许我们控制产生出的读序列的时间和空间局部性的程度。 *size* 越小，得到的工作集越小，因此时间局部性越好。 *stride* 越小，得到的空间局部性越好。那么我们就能得到一个读带宽的时间和空间局部性的二维函数，称为**存储器山**。

![](./pic/image-20231013001031609.png)